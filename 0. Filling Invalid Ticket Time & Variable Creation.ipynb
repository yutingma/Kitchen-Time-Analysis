{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store = 34\n",
    "w = ['Monday', 'Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'] \n",
    "data = pd.read_csv('Store'+str(Store)+'\\Store'+str(Store)+'_item_20180103-20190711.csv') \n",
    "data.head()\n",
    "\n",
    "#Define Invalid Criteria\n",
    "#Ticket Time absolute value: for example if TicketTime < 90 seconds, treat as invalid\n",
    "MinTicketTime = 90\n",
    "#Ticket Time in propportion to SentTime: for example if TicketTime < 0.3*SentTime, treat as invalid\n",
    "MinRatio = 0.3\n",
    "#Table Open Time Range: for example, treat Table Open Time between 30 and 240 minutes as valid\n",
    "MinTime = 30\n",
    "MaxTime  = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Type Cleaning, Filling Guest Count for OffSite Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Variable Format for Date-Time Variable\n",
    "data.loc[:,'BusinessDate'] = pd.to_datetime(data.loc[:,'BusinessDate'])\n",
    "\n",
    "date_time = ['OrderStartDateTime','NormalDateTime','BumpedDateTime','CookingDateTime']\n",
    "for i in date_time:\n",
    "    data.loc[:,i] = pd.to_datetime(data.loc[:,i], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "#Adjust Time Format\n",
    "data['StartTime'] = data['StartTime'].astype('str')\n",
    "data.loc[data['StartTime'].str.len()==3,'StartTime'] = '0'+data['StartTime']\n",
    "data.loc[data['StartTime'].str.len()==2,'StartTime'] = '00'+data['StartTime']\n",
    "data.loc[data['StartTime'].str.len()==1,'StartTime'] = '000'+data['StartTime']\n",
    "data['StartTime'] = data['StartTime'].str.slice(0,2)+':'+data['StartTime'].str.slice(2,5)\n",
    "\n",
    "#Add Hour, Half Hour, 15min Time Slot Identifier\n",
    "data['HalfHourStart'] = np.where(data['StartTime'].str.slice(3,5)=='15',data['StartTime'].str.slice(0,3)+'00', data['StartTime'])\n",
    "data['HalfHourStart'] = np.where(data['StartTime'].str.slice(3,5)=='45',data['StartTime'].str.slice(0,3)+'30', data['HalfHourStart'])\n",
    "\n",
    "data['QuarterHour'] = np.where(data['StartTime'].str.slice(0,2)=='00', \\\n",
    "                               24,(data['StartTime'].str.slice(0,2)).astype('int')) + data['StartTime'].str.slice(3,5).astype('int')/60\n",
    "data['QuarterHour'] = np.where(data['QuarterHour']<=4, data['QuarterHour']+24,data['QuarterHour'])\n",
    "\n",
    "data['HalfHour'] = data['QuarterHour']*4 //2 /2\n",
    "data['Hour'] = data['QuarterHour']*4//4\n",
    "\n",
    "#Adjust Time Slot Format\n",
    "data['DayHalfHour'] = data['BusinessDate'].dt.round('D') + pd.to_timedelta(data['HalfHourStart']+':00')\n",
    "data['DayQuarterHour'] = data['BusinessDate'].dt.round('D') + pd.to_timedelta(data['StartTime']+':00')\n",
    "data.loc[data['DayHalfHour'].dt.hour<=4,'DayHalfHour']=data['DayHalfHour']+pd.to_timedelta(1,'d')\n",
    "data.loc[data['DayQuarterHour'].dt.hour<=4,'DayQuarterHour']=data['DayQuarterHour']+pd.to_timedelta(1,'d')\n",
    "\n",
    "#Identify day of week: 0 represents Mon, 6 represents Sun \n",
    "data['DayOfWeek'] = data['BusinessDate'].dt.weekday\n",
    "data['DayOfWeekName'] = data['BusinessDate'].dt.weekday_name\n",
    "data['Week'] = data['BusinessDate'].dt.week\n",
    "data['Weekday'] = np.where(data['DayOfWeek']<=4,1,0)\n",
    "data['OrderTimeMin'] = data['OrderStartDateTime'].dt.round('T')\n",
    "\n",
    "#Identify holiday\n",
    "data['Holiday'] = np.where(data['NatHolidayDesc'].str.contains('/'),0,1)\n",
    "\n",
    "#Code Guest for Off-Site as -1\n",
    "data.loc[(data['ChannelName']=='Delivery')|(data['ChannelName']=='To Go Sales')|(data['ChannelName']=='Digital'),'GuestCount']=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Out-of-Operating Hour Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read operating hour from excel, with sheet name as Store number \n",
    "OperatingHour = pd.read_excel('Operating Hours.xlsx',sheet_name=str(Store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday</td>\n",
       "      <td>11</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>11</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>11</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>11</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday</td>\n",
       "      <td>11</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>10</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>10</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  Open  Close\n",
       "0     Monday    11   23.0\n",
       "1    Tuesday    11   23.0\n",
       "2  Wednesday    11   23.0\n",
       "3   Thursday    11   23.0\n",
       "4     Friday    11   24.5\n",
       "5   Saturday    10   24.5\n",
       "6     Sunday    10   23.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OperatingHour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Monday'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Monday'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2d5b546e98b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DayOfWeekName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HalfHour'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mOperatingHour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HalfHour'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mOperatingHour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1492\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    866\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m                 \u001b[0msection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m                 \u001b[1;31m# we have yielded a scalar ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no slices here, handle elsewhere'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3583\u001b[0m                                                       drop_level=drop_level)\n\u001b[0;32m   3584\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3585\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3587\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Monday'"
     ]
    }
   ],
   "source": [
    "#For everyday of the week, keep records if within operating hours. \n",
    "d=pd.DataFrame()\n",
    "for i in w:\n",
    "    d = d.append(data[((data['DayOfWeekName']==i)&(data['HalfHour']>=OperatingHour.loc[i,'Open'])&(data['HalfHour']<OperatingHour.loc[i,'Close']))])\n",
    "data = d \n",
    "del d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-07-11 00:00:00')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['BusinessDate'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-01-03 00:00:00')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['BusinessDate'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in Guest Count for Off-Premise Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of items in every check, update off-premise check's GuestCount with estimation (#items in check). \n",
    "GuestEst = data.groupby(['StoreKey','BusinessDate','CheckNum'])['ProductKey'].count().reset_index()\n",
    "GuestEst.columns = ['StoreKey','BusinessDate','CheckNum','GuestEst']\n",
    "data = data.merge(right=GuestEst, how = 'left',on=['StoreKey','BusinessDate','CheckNum'])\n",
    "data.loc[data['GuestCount']<=0,'GuestCount'] = data.loc[pd.isnull(data['GuestCount']),'GuestEst']\n",
    "data = data.drop(columns='GuestEst')\n",
    "del GuestEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['StoreKey', 'BusinessDate', 'DateKey', 'DayOfWeek', 'TimeKey',\n",
       "       'FullHour', 'HalfHour', 'QuarterHour', 'StartTime', 'Holiday',\n",
       "       'CheckNum', 'GuestCount', 'TableOpenMinutes', 'OpenHour', 'OpenMinute',\n",
       "       'CloseHour', 'CloseMinute', 'ChannelKey', 'TypeofServiceNum',\n",
       "       'ProductKey', 'CourseName', 'IXIName', 'MajorCodeName', 'MinorCodeName',\n",
       "       'StationKey', 'StationName', 'SentTime', 'OrderStartDateTime',\n",
       "       'NormalDateTime', 'CookingDateTime', 'BumpedDateTime', 'NatHolidayDesc',\n",
       "       'EmployeeKey', 'TicketTime', 'RNK', 'PROD_RNK', 'ORDER_RNK',\n",
       "       'TypeofServiceCat', 'ChannelName', 'StartTime.1', 'OffSiteOrder',\n",
       "       'OnSiteOrder', 'TotalOrder', 'OnSiteItem', 'OffSiteItem', 'TotalItem',\n",
       "       'HalfHourStart', 'Hour', 'DayHalfHour', 'DayQuarterHour',\n",
       "       'DayOfWeekName', 'Week', 'Weekday', 'OrderTimeMin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-Frequent Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Items sold less than 100 times\n",
    "data = data[data['ProductKey'].isin(list(data['ProductKey'].value_counts()[data['ProductKey'].value_counts()>=100].index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in Invalid Ticket Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define less than 100 seconds or less than 30% of SentTime as invalid TicketTime\n",
    "data['TicketTime'] = np.where((data['TicketTime']<=MinTicketTime)|(data['TicketTime']/data['SentTime']<MinRatio), np.NaN, data['TicketTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19646"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of Total Missing+Invalid Ticket Time\n",
    "pd.isnull(data['TicketTime']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define function for data filling ###\n",
    "#Paramaeter explanation: data type in parentheses. \n",
    "#dataset: (pandas dataframe)\n",
    "#objective: name of the target column to be filled. (string)\n",
    "#groupby: name of the columns used to group by and fill missing value. (list of strings)\n",
    "#measure: the measure taken from each group. (string: 'mean', 'median', or other pandas default aggregation function)\n",
    "#countcondition: criteria to use selected measure to fill in missing value. Measure will be adopted if calculated from number of valid records exceeding this criteria. \n",
    "#percentagecondition: criteria to use selected measure to fill in missing value. Measure will be adopted if valid records / total records exceeds this criteria. \n",
    "def fill_invalid(dataset, objective, groupby, measure, countcondition, percentagecondition): \n",
    "    group = dataset.groupby(groupby)[objective].agg({objective:[lambda x: x.shape[0],'count',measure]}).reset_index()\n",
    "    group.columns = groupby+['Count','CountValid',measure]\n",
    "    group = group[(group['CountValid']>=countcondition)&(group['CountValid']/group['Count'])>=percentagecondition]\n",
    "    d = dataset.merge(right=group, how='left',on=groupby)\n",
    "    d[objective] = np.where((pd.isnull(d[objective]))&(pd.isnull(d[measure])==False),d[measure],d[objective])\n",
    "    return list(d[objective])\n",
    "#returned object: list of values in the same order as input column, with missing value filled (if criterion met) and other values remain the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in Invalid for Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial #Missing\n",
    "pd.isnull(data[(data['Holiday']==1)]['TicketTime']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HalfHour 686\n",
      "Hour 519\n",
      "None 42\n"
     ]
    }
   ],
   "source": [
    "#Group by criteria: Product Key & HalfHour, then Product Key & Hour. \n",
    "for i in ['HalfHour','Hour',None]: \n",
    "    g = ['ProductKey','Holiday'] \n",
    "    if i != None:\n",
    "        g = g + [i]\n",
    "    data.loc[data['Holiday']==1,'TicketTime'] = fill_invalid(data[data['Holiday']==1],'TicketTime',g,'median',25,0.5)\n",
    "    #Pring the current group-by criteria and the result after filling (#missing)\n",
    "    print(i,pd.isnull(data[(data['Holiday']==1)]['TicketTime']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill In Invalid For Non-Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18894"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial #Missing\n",
    "pd.isnull(data[(data['Holiday']==0)]['TicketTime']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DayOfWeek HalfHour 8768\n",
      "DayOfWeek Hour 4142\n",
      "Weekday HalfHour 1946\n",
      "Weekday Hour 958\n"
     ]
    }
   ],
   "source": [
    "for i in ['DayOfWeek','Weekday']:\n",
    "    for j in ['HalfHour','Hour']: \n",
    "        g = ['ProductKey','Holiday']+[i]+[j]\n",
    "        data.loc[data['Holiday']==0,'TicketTime'] = fill_invalid(data[data['Holiday']==0],'TicketTime',g,'median',25,0.5)\n",
    "        print(i,j,pd.isnull(data[(data['Holiday']==0)]['TicketTime']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in the rest of invalid Ticket Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DayOfWeek HalfHour 989\n",
      "DayOfWeek Hour 983\n",
      "Weekday HalfHour 981\n",
      "Weekday Hour 928\n"
     ]
    }
   ],
   "source": [
    "for i in ['DayOfWeek','Weekday']:\n",
    "    for j in ['HalfHour','Hour']: \n",
    "        g = ['ProductKey']+[i]+[j]\n",
    "        data['TicketTime'] = fill_invalid(data,'TicketTime',g,'median',25,0.5)\n",
    "        print(i,j,pd.isnull(data['TicketTime']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in All the rest with Sent Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TicketTime'] = np.where(pd.isnull(data['TicketTime']),data['SentTime'],data['TicketTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check remaining missing value\n",
    "pd.isnull(data['TicketTime']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update BumpedDateTime from filled TicketTime\n",
    "data['BumpedDateTime_fill'] = data['NormalDateTime']+pd.to_timedelta(data['TicketTime'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19646"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of Items with updated BumpedDateTime\n",
    "(data['BumpedDateTime_fill']!=data['BumpedDateTime']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill In Invalid Table Open Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separting On/Off Premise Orders\n",
    "data['TypeofServiceNum'] = np.where(data['ChannelName'].isin(['Dining Room','Bar','Bar Dining','Patio']),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define less than 30 minutes and more than 300 minutes as invalid\n",
    "data['TableOpenMinutes'] = np.where((data['TypeofServiceNum']==0)&((data['TableOpenMinutes']<MinTime)|(data['TableOpenMinutes']>MaxTime)), np.NaN, data['TableOpenMinutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3743"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data[(data['TypeofServiceNum']==0)]['TableOpenMinutes']).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in Holiday Table Open Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data[(data['TypeofServiceNum']==0)&(data['Holiday']==1)]['TableOpenMinutes']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HalfHour 2\n",
      "Hour 1\n",
      "None 0\n"
     ]
    }
   ],
   "source": [
    "for i in ['HalfHour','Hour',None]: \n",
    "    g = ['GuestCount','Holiday'] \n",
    "    if i != None:\n",
    "        g = g + [i]\n",
    "    data.loc[(data['Holiday']==1)&(data['TypeofServiceNum']==0),'TableOpenMinutes'] = fill_invalid(data[(data['Holiday']==1)&(data['TypeofServiceNum']==0)],'TableOpenMinutes',g,'median',30,0.7)\n",
    "    print(i,pd.isnull(data[(data['Holiday']==1)&(data['TypeofServiceNum']==0)]['TableOpenMinutes']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in NonHoliday Table Open Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DayOfWeek HalfHour 104\n",
      "DayOfWeek Hour 85\n",
      "Weekday HalfHour 68\n",
      "Weekday Hour 60\n"
     ]
    }
   ],
   "source": [
    "for i in ['DayOfWeek','Weekday']:\n",
    "    for j in ['HalfHour','Hour']: \n",
    "        g = ['GuestCount','Holiday'] +[i,j]\n",
    "        data.loc[(data['Holiday']==0)&(data['TypeofServiceNum']==0),'TableOpenMinutes'] = fill_invalid(data[(data['Holiday']==0)&(data['TypeofServiceNum']==0)],'TableOpenMinutes',g,'median',30,0.7)\n",
    "        print(i,j,pd.isnull(data[(data['Holiday']==0)&(data['TypeofServiceNum']==0)]['TableOpenMinutes']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the rest of Table Open Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DayOfWeek HalfHour 60\n",
      "DayOfWeek Hour 60\n",
      "Weekday HalfHour 59\n",
      "Weekday Hour 59\n"
     ]
    }
   ],
   "source": [
    "#Fill in Holiday and Non-Holiday based on Day and Time \n",
    "for i in ['DayOfWeek','Weekday']:\n",
    "    for j in ['HalfHour','Hour']: \n",
    "        g = ['GuestCount'] +[i,j]\n",
    "        data.loc[(data['TypeofServiceNum']==0),'TableOpenMinutes'] = fill_invalid(data[(data['TypeofServiceNum']==0)],'TableOpenMinutes',g,'median',25,0.5)\n",
    "        print(i,j,pd.isnull(data[(data['TypeofServiceNum']==0)]['TableOpenMinutes']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#Try to Fill in All based on Party Size\n",
    "data.loc[(data['TypeofServiceNum']==0),'TableOpenMinutes'] = fill_invalid(data[(data['TypeofServiceNum']==0)],'TableOpenMinutes',['GuestCount'],'median',20,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if all values filled\n",
    "pd.isnull(data[(data['TypeofServiceNum']==0)]['TableOpenMinutes']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrush\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#Change Filling Criterion to Fill All based on Party Size\n",
    "data.loc[(data['TypeofServiceNum']==0),'TableOpenMinutes'] = fill_invalid(data[(data['TypeofServiceNum']==0)],'TableOpenMinutes',['GuestCount'],'median',5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Table Open and Close Time based on Table Open Duration. \n",
    "data['OpenDateTime'] = data['BusinessDate']+pd.to_timedelta(data['OpenHour'],unit='h')+pd.to_timedelta(data['OpenMinute'],unit='m')\n",
    "data['CloseDateTime'] = data['OpenDateTime']+pd.to_timedelta(data['TableOpenMinutes'],unit='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Concurrent with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate From Item-Level to Order & Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order: each row is one order with start and bump time. Used for #Concurrent Order counts. \n",
    "#Notice: batches of order within the same check is considered separate orders. \n",
    "#This is the base of JOINs, since concurrent counts are calculated based on order start time and items within an order shares the same count\n",
    "orders = data.sort_values(by='BumpedDateTime_fill',ascending=False).\\\n",
    "drop_duplicates(subset=['StoreKey','BusinessDate','CheckNum','OrderTimeMin'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table: each row is one on-premise check (table) with table open and close time. Used for #On-Premise Guest counts. \n",
    "guests = orders[orders['TypeofServiceNum']==0].sort_values(by='BumpedDateTime_fill', ascending=False).\\\n",
    "drop_duplicates(subset=['StoreKey','BusinessDate','CheckNum'],keep='first')[['StoreKey','BusinessDate','CheckNum','GuestCount','OpenHour','OpenMinute','TableOpenMinutes','BumpedDateTime_fill']]\n",
    "guests['OpenDateTime'] = guests['BusinessDate']+pd.to_timedelta(guests['OpenHour'],unit='h')+pd.to_timedelta(guests['OpenMinute'],unit='m')\n",
    "guests['CloseDateTime'] = guests['OpenDateTime']+pd.to_timedelta(guests['TableOpenMinutes'],unit='m')\n",
    "guests['CloseDateTime'] = np.where(guests['CloseDateTime']<=guests['BumpedDateTime_fill'],guests['BumpedDateTime_fill']+pd.to_timedelta(10,unit='m'),guests['CloseDateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only columns used in SQL JOIN\n",
    "orders = orders[['StoreKey','BusinessDate','OrderStartDateTime','BumpedDateTime_fill','OrderTimeMin','TypeofServiceNum','CheckNum','StationName']]\n",
    "guests = guests[['StoreKey','BusinessDate','OpenDateTime','CloseDateTime','GuestCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Create 'database' in memory\n",
    "conn = sqlite3.connect(':memory:')\n",
    "#Write talbes into the 'database'\n",
    "data[['StoreKey','BusinessDate','OrderStartDateTime','BumpedDateTime_fill','OrderTimeMin','TypeofServiceNum','CheckNum','StationName']].to_sql('BASE2', conn, index=False)\n",
    "orders.to_sql('ORDERS', conn, index=False)\n",
    "guests.to_sql('GUESTS', conn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join orders to the base: #Concurrent Orders\n",
    "qry = '''\n",
    "SELECT\n",
    "ORDERS.BusinessDate, ORDERS.CheckNum, ORDERS.OrderTimeMin, ORDERC.TypeofServiceNum,\n",
    "COUNT(ORDERC.CheckNum) 'ORDERCOUNT'\n",
    "FROM ORDERS \n",
    "JOIN ORDERS ORDERC \n",
    "\tON ORDERC.BusinessDate = ORDERS.BusinessDate \n",
    "\tAND ORDERC.StoreKey = ORDERS.StoreKey\n",
    "\tAND ORDERC.CheckNum <> ORDERS.CheckNum \n",
    "\tAND ORDERC.BumpedDateTime_fill > ORDERS.OrderTimeMin\n",
    "\tAND ORDERC.OrderStartDateTime <= ORDERS.OrderTimeMin\n",
    "GROUP BY ORDERS.BusinessDate, ORDERS.CheckNum, ORDERS.OrderTimeMin, ORDERC.TypeofServiceNum\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OrderCount = pd.read_sql_query(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Variable Type\n",
    "OrderCount['BusinessDate'] = pd.to_datetime(OrderCount['BusinessDate'])\n",
    "OrderCount['OrderTimeMin'] = pd.to_datetime(OrderCount['OrderTimeMin'])\n",
    "#Pivot the shape of OrderCount\n",
    "#End result: OnSite and OffSite Orders in separate columns\n",
    "OrderCount = OrderCount.groupby(['BusinessDate','CheckNum','OrderTimeMin','TypeofServiceNum'])['ORDERCOUNT'].mean().unstack().reset_index()\n",
    "OrderCount.columns=['BusinessDate','CheckNum','OrderTimeMin','OnSiteOrder_fill','OffSiteOrder_fill'] \n",
    "#Merge the Concurrent Counts back to data\n",
    "data = data.merge(right=OrderCount,how='left',on=['BusinessDate','CheckNum','OrderTimeMin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NA occurs if no other orders in process at the time point, fill with 0\n",
    "data['OnSiteOrder_fill']=data['OnSiteOrder_fill'].fillna(0)\n",
    "data['OffSiteOrder_fill']=data['OffSiteOrder_fill'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#OnSiteOrder Updated: 128207\n",
      "#OffSiteOrder Updated: 12412\n"
     ]
    }
   ],
   "source": [
    "#Number of records updated: \n",
    "print('#OnSiteOrder Updated:',(data['OnSiteOrder_fill'] != data['OnSiteOrder']).sum())\n",
    "print('#OffSiteOrder Updated:',(data['OffSiteOrder_fill'] != data['OffSiteOrder']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "SELECT\n",
    "ORDERS.OrderTimeMin, ORDERS.BusinessDate, ORDERS.CheckNum, PRODCOUNT.TypeofServiceNum,\n",
    "COUNT(PRODCOUNT.CheckNum) 'PRODCOUNT'\n",
    "FROM ORDERS \n",
    "JOIN BASE2 PRODCOUNT ON \n",
    "\tPRODCOUNT.BusinessDate = ORDERS.BusinessDate \n",
    "\tAND PRODCOUNT.StoreKey = ORDERS.StoreKey\n",
    "\tAND PRODCOUNT.CheckNum <> ORDERS.CheckNum \n",
    "\tAND PRODCOUNT.BumpedDateTime_fill > ORDERS.OrderTimeMin \n",
    "\tAND PRODCOUNT.OrderStartDateTime <= ORDERS.OrderTimeMin\n",
    "GROUP BY ORDERS.OrderTimeMin, ORDERS.BusinessDate, ORDERS.CheckNum, PRODCOUNT.TypeofServiceNum\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ProdCount = pd.read_sql_query(qry,conn)\n",
    "ProdCount['BusinessDate'] = pd.to_datetime(ProdCount['BusinessDate'])\n",
    "ProdCount['OrderTimeMin'] = pd.to_datetime(ProdCount['OrderTimeMin']) \n",
    "ProdCount = ProdCount.groupby(['BusinessDate','CheckNum','OrderTimeMin','TypeofServiceNum'])['PRODCOUNT'].mean().unstack().reset_index()\n",
    "ProdCount.columns=['BusinessDate','CheckNum','OrderTimeMin','OnSiteItem_fill','OffSiteItem_fill']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge concurrent count back to data\n",
    "data = data.merge(right = ProdCount,how='left',on=['BusinessDate','CheckNum','OrderTimeMin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NA occurs if no other items in process at the time point, fill with 0\n",
    "data['OnSiteItem_fill']=data['OnSiteItem_fill'].fillna(0)\n",
    "data['OffSiteItem_fill']=data['OffSiteItem_fill'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#OnSiteItem Updated: 315966\n",
      "#OffSiteItem Updated: 30372\n"
     ]
    }
   ],
   "source": [
    "#Number of records updated: \n",
    "print('#OnSiteItem Updated:',(data['OnSiteItem_fill'] != data['OnSiteItem']).sum())\n",
    "print('#OffSiteItem Updated:',(data['OffSiteItem_fill'] != data['OffSiteItem']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guest Count\n",
    "qry = '''\n",
    "SELECT\n",
    "ORDERS.OrderTimeMin, ORDERS.BusinessDate, ORDERS.CheckNum,\n",
    "SUM(GUESTCOUNT.GuestCount) 'OnSiteGuest'\n",
    "FROM ORDERS \n",
    "JOIN GUESTS GUESTCOUNT ON \n",
    "\tGUESTCOUNT.BusinessDate = ORDERS.BusinessDate \n",
    "\tAND GUESTCOUNT.StoreKey = ORDERS.StoreKey\n",
    "\tAND GUESTCOUNT.CloseDateTime > ORDERS.OrderTimeMin\n",
    "\tAND GUESTCOUNT.OpenDateTime <= ORDERS.OrderTimeMin\n",
    "GROUP BY ORDERS.BusinessDate, ORDERS.CheckNum, ORDERS.OrderTimeMin\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "GuestCount = pd.read_sql_query(qry,conn)\n",
    "GuestCount['BusinessDate'] = pd.to_datetime(GuestCount['BusinessDate'])\n",
    "GuestCount['OrderTimeMin'] = pd.to_datetime(GuestCount['OrderTimeMin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge concurrent count back to data\n",
    "data = data.merge(right = GuestCount,how='left',on=['BusinessDate','CheckNum','OrderTimeMin']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['OnSiteGuest'] = data['OnSiteGuest'].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProdCount = ProdCount.groupby(['BusinessDate','CheckNum','OrderTimeMin'])['PRODCOUNT'].mean().unstack().reset_index()\n",
    "#ProdCount.columns=['BusinessDate','CheckNum','OrderTimeMin','OnSiteItem_fill','OffSiteItem_fill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the original concurrent count before cleaning\n",
    "data['BumpedDateTime'] = data['BumpedDateTime_fill']\n",
    "data['OnSiteItem'] = data['OnSiteItem_fill']\n",
    "data['OffSsiteItem'] = data['OffSiteItem_fill']\n",
    "data['OnSiteOrder'] = data['OnSiteOrder_fill']\n",
    "data['OffSiteOrder'] = data['OffSiteOrder_fill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Store'+str(Store)+'\\Store'+str(Store)+'_item_filled_20180103-20190711.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent Count during 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the Time Frame Table\n",
    "#Each row is a Day-15min period\n",
    "time = data.groupby(['StoreKey','BusinessDate','TimeKey']).agg({'Holiday':'mean','DayQuarterHour':'first','QuarterHour':'mean','StartTime':'first'}).reset_index()\n",
    "time['DayQuarterHourEnd'] = time['DayQuarterHour']+pd.to_timedelta(15,unit='m') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Add time frame table to database, as the base for concurrent counts\n",
    "time.to_sql('DAYTIME',conn,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "SELECT\n",
    "DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, ORDERC.TypeofServiceNum, COUNT(ORDERC.CheckNum) 'ORDERCOUNT'\n",
    "FROM DAYTIME\n",
    "JOIN ORDERS ORDERC \n",
    "\tON DAYTIME.BusinessDate = ORDERC.BusinessDate\n",
    "\tAND DAYTIME.StoreKey = DAYTIME.StoreKey\n",
    "\tAND ORDERC.OrderStartDateTime < DAYTIME.DayQuarterHourEnd AND ORDERC.BumpedDateTime_fill > DAYTIME.DayQuarterHour\n",
    "GROUP BY DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, ORDERC.TypeofServiceNum\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OrderCount1 = pd.read_sql_query(qry,conn)\n",
    "OrderCount1['BusinessDate'] = pd.to_datetime(OrderCount1['BusinessDate'])\n",
    "OrderCount1['DayQuarterHour'] = pd.to_datetime(OrderCount1['DayQuarterHour']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderCount1 = OrderCount1.groupby(['BusinessDate','DayQuarterHour','TypeofServiceNum'])['ORDERCOUNT'].mean().unstack().reset_index()\n",
    "OrderCount1.columns= ['BusinessDate','DayQuarterHour','OnSiteOrder','OffSiteOrder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "SELECT\n",
    "DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, PROD.TypeofServiceNum, COUNT(PROD.CheckNum) 'PRODCOUNT'\n",
    "FROM DAYTIME\n",
    "JOIN BASE2 PROD ON DAYTIME.BusinessDate = PROD.BusinessDate\n",
    "\tAND PROD.OrderStartDateTime < DAYTIME.DayQuarterHourEnd AND PROD.BumpedDateTime_fill > DAYTIME.DayQuarterHour\n",
    "GROUP BY DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, PROD.TypeofServiceNum\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ProdCount1 = pd.read_sql_query(qry,conn)\n",
    "ProdCount1['BusinessDate'] = pd.to_datetime(ProdCount1['BusinessDate'])\n",
    "ProdCount1['DayQuarterHour'] = pd.to_datetime(ProdCount1['DayQuarterHour']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProdCount1 = ProdCount1.groupby(['BusinessDate','DayQuarterHour','TypeofServiceNum'])['PRODCOUNT'].mean().unstack().reset_index()\n",
    "ProdCount1.columns= ['BusinessDate','DayQuarterHour','OnSiteItem','OffSiteItem'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "SELECT\n",
    "DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, PROD.StationName, COUNT(PROD.CheckNum) 'PRODCOUNT'\n",
    "FROM DAYTIME\n",
    "JOIN BASE2 PROD ON DAYTIME.BusinessDate = PROD.BusinessDate\n",
    "\tAND PROD.OrderStartDateTime < DAYTIME.DayQuarterHourEnd AND PROD.BumpedDateTime_fill > DAYTIME.DayQuarterHour\n",
    "GROUP BY DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, PROD.StationName\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "StationCount1 = pd.read_sql_query(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "StationCount1['BusinessDate'] = pd.to_datetime(StationCount1['BusinessDate'])\n",
    "StationCount1['DayQuarterHour'] = pd.to_datetime(StationCount1['DayQuarterHour']) \n",
    "StationCount1 = StationCount1.groupby(['BusinessDate','DayQuarterHour','StationName'])['PRODCOUNT'].mean().unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "SELECT\n",
    "DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, SUM(GUESTCOUNT.GuestCount) 'OnSiteGuest'\n",
    "FROM DAYTIME\n",
    "JOIN GUESTS GUESTCOUNT ON DAYTIME.BusinessDate = GUESTCOUNT.BusinessDate\n",
    "\tAND GUESTCOUNT.OpenDateTime < DAYTIME.DayQuarterHourEnd AND GUESTCOUNT.CloseDateTime > DAYTIME.DayQuarterHour\n",
    "GROUP BY DAYTIME.BusinessDate, DAYTIME.DayQuarterHour\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "GuestCount1 = pd.read_sql_query(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "GuestCount1['BusinessDate'] = pd.to_datetime(GuestCount1['BusinessDate'])\n",
    "GuestCount1['DayQuarterHour'] = pd.to_datetime(GuestCount1['DayQuarterHour']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = time.merge(right=ProdCount1, how='left',on=['BusinessDate','DayQuarterHour'])\n",
    "time = time.merge(right=StationCount1, how='left',on=['BusinessDate','DayQuarterHour'])\n",
    "time = time.merge(right=OrderCount1, how='left',on=['BusinessDate','DayQuarterHour'])\n",
    "time = time.merge(right=GuestCount1, how='left',on=['BusinessDate','DayQuarterHour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = time.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.to_csv('Store'+str(Store)+'\\Store'+str(Store)+'_day-15min.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent Count during 15min: Bottoleneck Item in Each Order."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time1 = data.groupby(['BusinessDate','TimeKey']).agg({'Holiday':'mean','DayQuarterHour':'first','QuarterHour':'mean','StartTime':'first'}).reset_index()\n",
    "time1['DayQuarterHourEnd'] = time1['DayQuarterHour']+pd.to_timedelta(15,unit='m')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "qry = '''\n",
    "SELECT\n",
    "DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, ORDERS.StationName, COUNT(ORDERS.CheckNum) 'PRODCOUNT'\n",
    "FROM DAYTIME\n",
    "JOIN ORDERS ON DAYTIME.BusinessDate = ORDERS.BusinessDate\n",
    "\tAND ORDERS.OrderStartDateTime < DAYTIME.DayQuarterHourEnd AND ORDERS.BumpedDateTime_fill > DAYTIME.DayQuarterHour\n",
    "GROUP BY DAYTIME.BusinessDate, DAYTIME.DayQuarterHour, ORDERS.StationName\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "StationCount = pd.read_sql_query(qry,conn)\n",
    "StationCount['BusinessDate'] = pd.to_datetime(StationCount['BusinessDate'])\n",
    "StationCount['DayQuarterHour'] = pd.to_datetime(StationCount['DayQuarterHour']) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "StationCount = StationCount.groupby(['BusinessDate','DayQuarterHour','StationName'])['PRODCOUNT'].mean().unstack().reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time1 = time1.merge(right=ProdCount, how='left',on=['BusinessDate','DayQuarterHour'])\n",
    "time1 = time1.merge(right=StationCount, how='left',on=['BusinessDate','DayQuarterHour'])\n",
    "time1 = time1.merge(right=OrderCount, how='left',on=['BusinessDate','DayQuarterHour'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time1 = time1.fillna(0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time1.to_csv('Store'+str(Store)+'\\Store'+str(Store)+'_day-15min_bottleneck.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
